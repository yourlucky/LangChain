from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

from login_f import check_password

if not check_password():
    st.stop()

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOpenAI(
    temperature=0.2,
    #model="gpt-4o-mini",
    model='gpt-4',
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)


@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )


def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.
            
            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)


st.title("Document Chat")

st.markdown(
    """
Use this chatbot to toss any questions you have about your files straight to an AI.  
We've tweaked it to confidently say "I don't know" if it's unsure, keeping those AI hallucinations in check.

Why did I make this? Well, I was totally over reading endless reports and speaking for my boss. ğŸ˜  
Let the chatbot do the heavy lifting!


Please upload your files using the sidebar.

"""
)

st.markdown("""
<style>
.custom-font {
    font-size: 14px;  /* í°íŠ¸ í¬ê¸° ì¡°ì ˆ */
    font-weight: normal;  /* êµµê¸°ë¥¼ ì¼ë°˜ìœ¼ë¡œ ì„¤ì • */
}
</style>
<div class='custom-font'>
    ë³´ê³ ì„œë¥¼ ì½ê¸° ì‹«ì€ ë¶„ë“¤ì„ ìœ„í•´ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. í• ë£¨ì‹œë„¤ì´ì…˜ íš¨ê³¼ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë¥¸ë‹¤ê³  í•  í™•ë¥ ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True)


with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )

if file:
    retriever = embed_file(file)
    send_message("I'm ready! Ask away!", "ai", save=False)
    paint_history()
    message = st.chat_input("Ask anything about your file...")
    if message:
        send_message(message, "human")
        #docs=retriever.invoke(message)
        #docs=" \n\n".join(document.page_content for document in docs)
        #prompt = ChatPromptTemplate.from_messages(context=docs, question=message)
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)


else:
    st.session_state["messages"] = []