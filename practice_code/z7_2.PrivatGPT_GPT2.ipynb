{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 287/287 [00:00<00:00, 365kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 7.10MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 1.38MB/s]\n",
      "Downloading config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 5.06MB/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 16.9k/16.9k [00:00<00:00, 7.99MB/s]\n",
      "Downloading (…)l-00001-of-00002.bin: 100%|██████████| 9.95G/9.95G [02:38<00:00, 62.7MB/s]\n",
      "Downloading (…)l-00002-of-00002.bin: 100%|██████████| 4.48G/4.48G [01:10<00:00, 63.8MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [03:49<00:00, 114.78s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:15<00:00, 67.97s/it] \n",
      "Downloading generation_config.json: 100%|██████████| 117/117 [00:00<00:00, 6.77kB/s]\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    #model_id=\"tiiuae/falcon-7b-instruct\",#GPU없이는 너무 느림\n",
    "    task=\"text-generation\",\n",
    "    device=-1,#GPU=0, CPU=-1\n",
    "    pipeline_kwargs={\"max_new_tokens\":100},\n",
    ")\n",
    "#llm.client.api_url = 'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1'\n",
    "chain = prompt | llm\n",
    "\n",
    "result =chain.invoke({\"word\": \"tomato\"})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TheThird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
